# -*- coding: utf-8 -*-
"""BERT_Twitter_Disaster_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QNGPcMLOwtW5hw0sGmr50scJgntNwzBx
"""

import pandas as pd
import numpy as np

import re
import string
import os


import matplotlib.pyplot as plt
import seaborn as sns

from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

plt.style.use('ggplot')

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout
from tensorflow.keras.optimizers import Adam

tweet.to_csv('Twitter_disaster_data.csv')

tweet = pd.read_csv('Twitter_disaster_data.csv')

"""# Exploratory Data Analysis & Features Extraction

"""

tweet.head()

tweet.shape

tweet.info()

#Exploratory Data Analysis

plt.rcParams['figure.figsize'] = [10,5]
plt.rcParams['figure.dpi'] = 120

##Target class distribution
sns.countplot(x='target' , data=tweet)
plt.title('Real or Wrong disaster tweet')

tweet['target'].value_counts()

tweet['target'].value_counts().plot.pie(autopct='%1.2f%%')

!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git

import preprocess_kgptalkie as kgp

print(type(tweet))

tweet=kgp.get_basic_features(tweet)

tweet.head()

sns.distplot(tweet['char_counts'])

sns.kdeplot(tweet['char_counts'])

sns.kdeplot(tweet[tweet['target']==1]['char_counts'],shade=True,color='red')
sns.kdeplot(tweet[tweet['target']==0]['char_counts'],shade=True,color='magenta')

sns.kdeplot(tweet[tweet['target']==1]['word_counts'],shade=True)
sns.kdeplot(tweet[tweet['target']==0]['word_counts'],shade=True)

sns.kdeplot(tweet[tweet['target']==1]['avg_wordlength'],shade=True)
sns.kdeplot(tweet[tweet['target']==0]['avg_wordlength'],shade=True)

sns.kdeplot(tweet[tweet['target']==1]['stopwords_counts'],shade=True)
sns.kdeplot(tweet[tweet['target']==0]['stopwords_counts'],shade=True)

freqs = kgp.get_word_freqs(tweet,'text')

type(freqs)

top20=freqs[:20]

plt.bar(top20.index,top20.values)

least20 = freqs[-20:]
least20

bigram=kgp.get_ngram(tweet,'text',ngram_range=2)

bigram[:20]

type(bigram)

#one shot data cleaning

def get_clean(x):
  x=str(x).lower().replace('\\',' ').replace('-',' ').replace('.',' ')
  #contraction to expansion like i'm to i am
  # x=kgp.cont_exp(x)
  x=kgp.remove_emails(x)
  x=kgp.remove_urls(x)
  x=kgp.remove_html_tags(x)
  x=kgp.remove_rt(x)
  x=kgp.remove_accented_chars(x)
  x=kgp.remove_special_chars(x)
  x=kgp.remove_dups_char(x)
  return x

# type(tweet['text'])
# print(len(tweet['text']))
# print(tweet['text'][2])
# for i in range(len(tweet['text'])):
#   # x = tweet['text'][i] 
#   x  = str(tweet['text'][i]).lower().replace('\\',' ').replace('-',' ').replace('.',' ')
#   # x=kgp.cont_exp(x)
#   x=kgp.remove_emails(x)
#   x=kgp.remove_urls(x)
#   x=kgp.remove_html_tags(x)
#   x=kgp.remove_rt(x)
#   x=kgp.remove_accented_chars(x)
#   x=kgp.remove_special_chars(x)
#   x=kgp.remove_dups_char(x)
#   tweet['text'][i] = x
# get_clean(str(tweet['text'][3]))
# print(type((tweet['text'][3])))
# tweet['text'][3] = tweet['text'][3].decode('utf-8')

tweet['text'] = tweet['text'].apply(lambda x : get_clean(x))

tweet.head()['text']

real=kgp.get_word_freqs(tweet[tweet['target']==1],'text')
real

real=' '.join(real.index)
real

word_cloud=WordCloud(max_font_size=100).generate(real)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

nreal=kgp.get_word_freqs(tweet[tweet['target']==0],'text')
nreal

nreal=' '.join(nreal.index)
nreal

word_cloud=WordCloud(max_font_size=100).generate(nreal)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

"""# Classification using TFIDF + SVM

"""

text=tweet['text']
y=tweet['target']

tfidf=TfidfVectorizer()
X=tfidf.fit_transform(text)

X.shape
X[4]

print(X.shape)

print(type(X))
print(X[2])

print(text)

print(y)

X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.2,random_state=0, stratify=y)

def run_SVM(clf, X_train , X_test , y_train , y_test):
  clf.fit(X_train,y_train)
  y_pred = clf.predict(X_test)

  print()
  print('Classification Report')
  print(classification_report(y_test,y_pred))

from sklearn.svm import LinearSVC
clf= LinearSVC()

"""Classification Report

"""

run_SVM(clf,X_train , X_test , y_train , y_test)

"""# DISTILBERT model building and training

"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install ktrain

tweet.head()

from ktrain import text
import ktrain

text.print_text_classifiers()

(train,val,preproc) = text.texts_from_df(train_df=tweet,text_column='text',label_columns='target', maxlen=512,preprocess_mode='distilbert')

# model=text.text_classifier(name='bert',train_data=(X_train, y_train), preproc=preproc)
model=text.text_classifier(name='distilbert',train_data=train,preproc=preproc)

# learner=ktrain.get_learner(model=model,train_data=(X_train,y_train),val_data=(X_test,y_test),batch_size=64)
learner=ktrain.get_learner(model=model,
                           train_data=train,
                           val_data=val,
                           batch_size=6)

learner.fit_onecycle(lr=2e-5,epochs=10)

#find out best learning rate
#learner.lr_find()
#learner.lr_plot()

predictor=ktrain.get_predictor(learner.model,preproc)
# predictor.save('/content/distilbert')

from google.colab import drive
drive.mount('/content/drive')

predictor.save('/content/drive/MyDrive/distilbert')

"""Testing/Predictions

"""

predictor=ktrain.get_predictor(learner.model,preproc)
data=['i met you today by accident','i got today car accident , i am injured']
predictor.predict(data)

classes=predictor.get_classes()
print(classes)

y_pred=predictor.predict(data[1])
classes.index(y_pred)